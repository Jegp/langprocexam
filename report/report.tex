\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{filecontents}

\title{Comparing regression and LSTM sentiment analysis models}

\author{Language Processing II exam \\ Jens Egholm Pedersen \\ \texttt{xtp778@sc.ku.dk}}

\date{\today}

\begin{document}
\maketitle

\tableofcontents

\pagebreak

\section{Introduction}
\label{sec:introduction}
Due to the enormous rise in accessible data and processing capabilities,
computers have been moving from the restricted formalized domain of
mathematics and computability, and into the more complex and seemingly
chaotic domain of natural language \citep{NILSSON2009, Jurafsky2000}. Automating
understanding of natural language have a wide range of helpful applications,
ranging from translation to recommender systems to generic personal assistants
\citep{COX2005, BKL2009}.

As a part of this development, sentiment analysis has been attracting increased
attention \citep{BKL2009, Jurafsky2000}.
%%%
%%% TODO: Comment on why you chose linear + NN
%%% Recent advancement in LSTM networks... Used in Google etc.
%%%

In the present paper, sentiment
analysis is applied to a corpus of movie reviews presented by \cite{PangLee2005}.
By using standard natural
language processing tools and machine learning techniques, four models are built
and trained to predict the rating of a movie review on a discrete normalized scale.

\section{Background}
This section illuminates the theory and background for language processing
and basic methodology for processing text.
Sentiment analysis as a fundamental concept for this setting will be introduced
second, followed by fundamental statistical metrics. Lastly,
recurrent neural networks will be introduced before moving on to the data
generation.

\subsection{Language processing}
Following the revolutionizing formalizations of grammar by \cite{Chomsky2002},
first written in 1957, much work was put into working with the computability
of language \citep{Jurafsky2000}. In the beginning of the 21st century,
the previous work converged with the statistical machine learning community,
giving rise to supervised, semi-supervised and even unsupervised models
of language understanding and translation \citep{Jurafsky2000}.

There are several approaches to language processing, but it is common
to deconstruct text into its constituents and assign meaning
to each piece, with the hope that the disassembly gives additional meaning
that can aggregates to an understanding of the text as a whole when the pieces
are put back together \citep{Jurafsky2000}.
\textit{Constituents} can range from morphemes to words to whole sentences in
this context. This processes is referred to as feature extraction due to the
enrichment of the text with meta-data.

A fundamental method for feature extraction is to construct a probabilistic
model by simply joining items (ranging from phonemes to word pairs) together
in pairs of one, two or more, called \textit{n}-grams \citep{Jurafsky2000}.
The advantage of this approach focuses on speed and the simple, yet efficient,
probabilites in for instance word pairs which can be applied to predict
likehood of future occurences \citep{Jurafsky2000}.

Term frequency, inverse document frequency (tf-idf) is another relevant feature
that describes the relevance of an item, weighted against the number of
times it appears in other documents \citep{Jurafsky2000} (see figure \ref{fig:tfidf}).
Tf-idf favours words that is used heavily in a
single document (assumed to be important) while diminishing the importance of
words that often occur in other documents, and is a heavily used metric \citep{Jurafsky2000}.

\begin{figure}
\[tf-idf = Nw \cdot log(\frac{N}{k})\]
\caption{Formula for tf-idf.}
\label{fig:tfidf}
\end{figure}

\subsubsection{Sentiment analysis}
Another popular approach for disassembling text
called part of speech-tagging (POS tagging), assigns grammatical categories
to each word and attempt to build a linguistic model that can be understood by
a machine \citep{Jurafsky2000}. This works well for information retrieval,
where objective truths can be extracted\footnote{Such as the objective statements about an object in the
sentence "\textit{The cat likes tuna fish}".}, but fails
to capture the complexity and emotional variety of sentences that appear
outside the grammar, such as emotions and opinions\footnote{One example is a heavily sarcastic sentence like
"\textit{I'd really truly love going out in this weather!}". For POS-tagging
sarcasm is difficult to capture because it is not implicitly encoded in the
grammatical categories.} \citep{Jurafsky2000}.

Sentiment analysis focuses on extracting information about the emotions or
opinions of a text \citep{PangLee2008}.
Defining what a \textit{sentiment} is not a simple task, and have been
scrutinized extensively \citep{Jurafsky2000, PangLee2008}.
A much used approach was introduced by \cite{Ekman92} where he divised six
basic emotions, which could be conveniently operationalized by computers.
Since the dataset for this paper revolves around opinions in a single dimension,
\textit{sentiment} will in this paper simply refer to a subjective experience,
discretized to a number (see section \ref{sec:dataset}).

\subsection{Linear regression}
Linear regression is a mathematical function that describes how the mean of the
output changes with the value of the input \citep{Agresti2008}. The model is
appropriate when a linear correlation between the input and output can be
assumed, so an increase in the input results in a linear in- or decrease in
the output, represented by a straight line in a 2-dimensional coordinate system
\citep{Agresti2008}.

\subsubsection{Error reporting}
The predictions made by linear regression model ($y$), does not always fit
the actual values ($ŷ$). To describe this error and to give indicators on the
quality of the model, the mean absolute error (MAE) and root mean squared
error (RMSE) is introduced \citep{Agresti2008, Chai2014}.

MAE in figure \ref{fig:mae} describes the mean of the absolute error values.
If a model has an MAE of 1, the average distance to the actual value from the
predicted value is 1. The bigger the mean error value, the worse the model is.

\begin{figure}
\[MAE = \frac{\Sigma |e|}{n}, e = y - ŷ\]
\caption{Mean absolute error (MAE).}
\label{fig:mae}
\end{figure}

However, this figure does not capture the error distribution of the model
\citep{Chai2014}. If
a gaussian distribution is assumed for the errors, it is important to know
the standard distribution to indicate \textit{how} consistenly wrong the model
is \citep{Agresti2008}. The root mean square error measures the standard
deviation in the sample of the errors, making it a good description for the
model accuracy, compared to other models \citep{Agresti2008, Chai2014}.

\begin{figure}
\[ RMSE = \sqrt{\frac{\Sigma (e)^{2}}{n - 2}}, e = y - ŷ \]
\caption{Root mean square error (RMSE).}
\label{fig:rmse}
\end{figure}

Each metric should not stand alone however, and MAE and RMSE is typically
reported together \citep{Chai2014}.

\subsection{Neural networks}
Inspired by the biological brain, models for neural networks have been applied
in computer science the 1950s, and have become integral part of machine learning
 \citep{NILSSON2009, Russell2009}.
Neurons are essentially functions that operate on some input and respond with
some output \citep{Russell2009}. The inputs for a neuron are weighted to give
different input channels - or input dimensions - varying significance. These
weighted inputs arrive to an activation function that decides whether the
neuron should ‘fire’ or not \citep{NILSSON2009}. Sigmoid or hyperbolic tangent
are popular
activation functions for numerical predictions because of their steep logistic
properties while retaining differentiability. Neural networks excell by their
adaptability and have long been applied to the field of language processing
\citep{Jurafsky2000}.

Simple neural networks can be stacked in layers, where each neuron will assign
weights to the input, to determine its significance for the activation function
\citep{NILSSON2009}. By tuning the weights of the neurons, such groupings can
learn to fire on certain input patterns \citep{Russell2009}. However,
this structure have proved brittle and difficult to train because the
neurons do not retain their weights for long \citep{NILSSON2009, Russell2009}.
\cite{Rumelhart1988} suggested a method to avoid this instability by adjusting
the weights of the neurons in retrospect with a method called back-propagation
\citep{Rumelhart1988, NILSSON2009}. This optimization is
operationalized by a function that can describe the \textit{loss} of effiency
in a network, and then adjust the weights correspondingly using gradient
descent \citep{Russell2009}.

\subsubsection{Long short-term memory networks}
Back-propagating networks with a high complexity in layers and neurons have
shown to be hard to optimize above a certain point, because the gradient
descent algorithm are inefficient at escaping local extrema in a high-dimensional
space \citep{Russell2009}. Recurrence have been used to counter this problem,
by conserving \emdash or returning to \emdash previous, optimal values
\citep{Schmidhuber2015, Russell2009}. \cite{Hochreiter1997} suggested a certain
type of memory-retaining networks that is able to balance between both
storing previous optimal state, while also forgetting context when new and
different input arrives \citep{Hochreiter1997, Schmidhuber2015}.
They dubbed the model long short-term memory (LSTM) networks, to capture both
properties \citep{Hochreiter1997}.

\begin{figure}
  \centering
  \includegraphics[width=0.8\textwidth]{lstm.png}
  \caption{An \textit{peephole} LSTM unit with three gates (input, output and forget).}
  \label{fig:lstm}
\end{figure}

The LSTM model seen in figure \ref{fig:lstm} builds on the idea of a more
complex neuronal unit, with several \textit{gates}, collaborating to give
the LSTM the desired property \citep{Gers2001}. A traditional unit contains
three gates: an "input" gate that control the degree with which new information
should influence the current memory state, a "forget" gate that decides whether
a value should be retained and an output gate that decides how much influence
the memory should have on the activation of the unit \citep{Hochreiter1997, Gers2001}.
There are a plentitude of other architectures, but for this paper, the LSTM
model shown in figure \ref{fig:lstm} is used \citep{Gers2001}.

\subsubsection{Optimisation and regularization}
Recurrent neural networks are typically trained with a continuously applied
loss function and gradiend descent for weight correction \citep{Russell2014,
Hochreiter1997, Schmidhubner2015}. Stochastic gradiens descent and resilient
back-propagation are two common approaches, which have proved efficient
\citep{Russell2014, NILSSON2009}. This paper applies a third method, based on
the RMSE algorithm in figure \ref{fig:rmse}, called root mean square propagation
(RMSprop), which preserves the stochastic properties while controlling
for previous errors in the network \citep{Tieleman2012}.

LSTMs and machine learning models in general can overfit to the data
they are trained on, loosing their ability to predict data outside the training
set \citep{Russell2014, NILSSON2009}.
A common approach to avoid overfitting is to introduce additional
information in the learning process, so the model gets sufficiently confused
to lose any overfitted optimizations, but sufficiently focused to only save
the relevant parameters \citep{Schmidhubner2015, NILSSON2009}. For LSTM networks
a common approach is simply to set a random part of the data to zero
\citep{Schmidhubner2015}. This type of data dropout is applied later in the
paper.

\subsubsection{Error reporting}
Similar to the linear regression model, the LSTMs are predicting a numerical
value (see \ref{sec:dataset}). For this type of prognoses MAE and RMSE are
commonly used as indicators of the model accuracy
\citep{Russel2014, NILSSON2009, Schmidhubner2015}. As with the linear model,
they are capable of capturing both the average error in absolute terms,
and the deviation from the mean of the expected predictions.

\section{Data generation}

\subsubsection{Data set}
\label{sec:dataset}
Chose to focus on ratings

\subsection{Learning models}
The four models model is built using the Scikit Learn (Sklearn) package and second
is based on the deep learning library Keras (see appendix in section \ref{app:B}).
Two The corpus is evaluated against four different implementations of
sentiment analysis, based on two different models. The first model uses the

\subsubsection{K-fold cross validation}
The dataset

N-grams \cite{Jurafsky2000}.

max features for bow (10'000)

this section must
report on actual test-runs utilizing the movie review corpus and applying at least two
machine learning algorithms (Naïve-Bayes being one example)

\begin{figure}
  \begin{centering}
    \begin{tabular}{ l c c c }
      \textbf{Model} & \textbf{r\textsuperscript{2}} & \textbf{MAE} & \textbf{RMSE} \\ \hline
      Linear          & 0   & 0.1649 & 0.2075 \\
      Linear + tf-idf & 0   & 0.1494 & 0.1855 \\
      LSTM            & N/A & 0.1667 & 0.2045 \\
      LSTM + tf-idf   & N/A & 0.1641 & 0.2014
    \end{tabular}
    \caption{Comparison of metrics for the four different models using mean absolute error (MAE) and root mean squared error (RMSE).}
  \end{centering}
\end{figure}

\section{Quantitative evaluation}
demonstrating how SA results can be evaluated
automatically

\section{Qualitative evaluation}
evaluating and discussing the quantitative results wrt.
validity, reliability and/or relevance in a “real
-world” perspective

The very definition of \textit{sentiment} in section \ref{ref:background} as
a subjective opinion makes comparison difficult. As stated in \citep{PangLee2005}
the entries in the present dataset is difficult to compare upon, since ratings
are relative between reviewers; a rating of 0.8 might be high for one reviewer,
but low for another. Unfortunately a simple normalization does not solve the
problem because the scale of one reviewer might not even be linearly
comparable to another.

\subsection{Other approaches}
Not taken
AFINN, leksika
word2vec
\cite{IMM2011-06010}

\section{Conclusion}
presenting in a condensed form your results and observations, e.g.
pointing to strengths, weaknesses, and future directions

\clearpage% or cleardoublepage
\renewcommand*{\refname}{}
\section{References}

\bibliographystyle{apalike}
\bibliography{report}

\begin{filecontents}{report.bib}
  @MISC\{IMM2011-06010,
      author       = "F. {\AA}. Nielsen",
      title        = "AFINN",
      year         = "2011",
      month        = "mar",
      keywords     = "word list, sentiment analysis, opinion mining, text mining",
      publisher    = "Informatics and Mathematical Modelling, Technical University of Denmark",
      address      = "Richard Petersens Plads, Building 321, {DK-}2800 Kgs. Lyngby",
      url          = "http://www2.imm.dtu.dk/pubdb/p.php?6010",
  },
  @book{Russell2009,
   author = {Russell, Stuart and Norvig, Peter},
   title = {Artificial Intelligence: A Modern Approach},
   year = {2009},
   isbn = {0136042597, 9780136042594},
   edition = {3rd},
   publisher = {Prentice Hall Press},
   address = {Upper Saddle River, NJ, USA},
  },
  @incollection{Rumelhart1988,
   author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
   chapter = {Learning Representations by Back-propagating Errors},
   title = {Neurocomputing: Foundations of Research},
   editor = {Anderson, James A. and Rosenfeld, Edward},
   year = {1988},
   isbn = {0-262-01097-6},
   pages = {696--699},
   numpages = {4},
   url = {http://dl.acm.org/citation.cfm?id=65669.104451},
   acmid = {104451},
   publisher = {MIT Press},
   address = {Cambridge, MA, USA},
  },
  @article{COX2005,
    title = "Metacognition in computation: A selected research review",
    journal = "Artificial Intelligence",
    volume = "169",
    number = "2",
    pages = "104 - 141",
    year = "2005",
    note = "Special Review Issue",
    issn = "0004-3702",
    doi = "http://dx.doi.org/10.1016/j.artint.2005.10.009",
    url = "http://www.sciencedirect.com/science/article/pii/S0004370205001530",
    author = "Michael T. Cox"
  },
  @book{NILSSON2009,
    author = "Nils J. Nilsson",
    title = "The quest for artificial intelligence - A history of ideas and achievements",
    year = "2009",
    publisher = "Cambridge University Press"
  },
  @inproceedings{PangLee2005,
    author = {Bo Pang and Lillian Lee},
    title = {Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales},
    year = {2005},
    pages = {115--124},
    booktitle = {Proceedings of ACL}
  },
  @article{PangLee2008,
    author = {Bo Pang and Lillian Lee},
    title = "Opinion mining and sentiment analysis",
    year = "2008",
    journal = "Foundations and Trends in Information Retrieval",
    volume = 2,
    pages = {1-135}
  }
  @book{BKL2009,
    added-at = {2016-12-06T16:29:36.000+0100},
    address = {Beijing},
    author = {Bird, Steven and Klein, Ewan and Loper, Edward},
    biburl = {https://www.bibsonomy.org/bibtex/2c90dc59441d01c8bef58a947274164d4/flint63},
    doi = {http://my.safaribooksonline.com/9780596516499},
    file = {O'Reilly eBook:2009/BirdKleinLoper09.pdf:PDF;O'Reilly Product page:http\://shop.oreilly.com/product/9780596516499.do:URL;Related Web Site:http\://www.nltk.org/:URL},
    groups = {public},
    interhash = {5408d7da097b9cd81239c238da8bfaf4},
    intrahash = {c90dc59441d01c8bef58a947274164d4},
    isbn = {978-0-596-51649-9},
    keywords = {01624 103 safari book ai software development language processing text python framework},
    publisher = {O'Reilly},
    timestamp = {2016-12-06T16:29:36.000+0100},
    title = {Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit},
    url = {http://www.nltk.org/book},
    username = {flint63},
    year = 2009
  },
  @MISC{Gers2001,
      author = {Felix Gers},
      title = {Long Short-Term Memory in Recurrent Neural Networks},
      year = {2001}
  },
  @article{Hochreiter1997,
    author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
    title = {Long Short-Term Memory},
    journal = {Neural Comput.},
    issue_date = {November 15, 1997},
    volume = {9},
    number = {8},
    month = nov,
    year = {1997},
    issn = {0899-7667},
    pages = {1735--1780},
    numpages = {46},
    url = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
    doi = {10.1162/neco.1997.9.8.1735},
    acmid = {1246450},
    publisher = {MIT Press},
    address = {Cambridge, MA, USA},
  },
  @book{Jurafsky2000,
   author = {Jurafsky, Daniel and Martin, James H.},
   title = {Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},
   year = {2000},
   isbn = {0130950696},
   edition = {1st},
   publisher = {Prentice Hall PTR},
   address = {Upper Saddle River, NJ, USA},
  },
  @article{Chai2014,
     author = {Chai, T. and {Draxler}, R.~R.},
      title = "{Root mean square error (RMSE) or mean absolute error (MAE)? - Arguments against avoiding RMSE in the literature}",
    journal = {Geoscientific Model Development},
       year = 2014,
      month = jun,
     volume = 7,
      pages = {1247-1250},
        doi = {10.5194/gmd-7-1247-2014},
     adsurl = {http://adsabs.harvard.edu/abs/2014GMD.....7.1247C},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
  }.
  @book{chomsky2002,
    title={Syntactic Structures},
    author={Chomsky, N.},
    isbn={9783110172799},
    lccn={2002043087},
    series={Mouton classic},
    url={https://books.google.dk/books?id=a6a\_b-CXYAkC},
    year={2002},
    publisher={Bod Third Party Titles}
  },
  @ARTICLE{Ekman92,
    author = {Paul Ekman},
    title = {An argument for basic emotions},
    journal = {Cognition and Emotion},
    year = {1992},
    pages = {169--200}
  },
  @book{Agresti2008,
    author = {Agresti, Alan and Finlay, Barbara},
    citeulike-article-id = {5485960},
    day = {07},
    edition = {4},
    howpublished = {Hardcover},
    isbn = {0130272957},
    keywords = {methodology, quantitative, statistics},
    month = jan,
    posted-at = {2009-08-19 11:15:48},
    priority = {2},
    publisher = {Prentice Hall},
    title = {{Statistical Methods for the Social Sciences (4th Edition)}},
    url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0130272957},
    year = {2008}
  },
  @article{Schmidhuber2015,
    title = "Deep learning in neural networks: An overview ",
    journal = "Neural Networks ",
    volume = "61",
    number = "",
    pages = "85 - 117",
    year = "2015",
    note = "",
    issn = "0893-6080",
    doi = "https://doi.org/10.1016/j.neunet.2014.09.003",
    url = "http://www.sciencedirect.com/science/article/pii/S0893608014002135",
    author = "Jürgen Schmidhuber",
    keywords = "Deep learning",
    keywords = "Supervised learning",
    keywords = "Unsupervised learning",
    keywords = "Reinforcement learning",
    keywords = "Evolutionary computation "
  }.
  @other{Tieleman2012,
    author = {Tieleman, Tijmen and Hinton, Geoffrey},
    title = "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude",
    publisher = "Coursera: Neural Networks for Machine Learning",
    year = 2012,
  }
\end{filecontents}

\pagebreak
\section{Appendix A: Data}
\label{app:A}
The data used in this report is collected from \cite{PangLee2005}. The data
is available in the software repository (see \ref{app:B}). The readme with
further descriptions on the data source is available at:

\url{http://www.cs.cornell.edu/people/pabo/movie-review-data/scaledata.README.1.0.txt}

\pagebreak
\section{Appendix B: Software}
\label{app:B}

All software used to produce the results presented in this report is open-source
and available through GitHub at:

\URL{https://github.com/Jegp/langprocexam}

Below follows a list of the software used. An in-depth description on how
to reproduce the results are availadle from the GitHub repository above.

\subsection{NLTK}
A toolkit for natural language processing in python. Visited 15th of June 2017.

\URL{http://www.nltk.org/api/nltk.sentiment.html}

\subsection{Scikit-learn}
Machine learning library for Python. Visited 15th of June 2017.

\URL{http://scikit-learn.org/stable/index.html}

\subsection{Tensorflow}
A machine learning library initially developed by Google Inc.
Visited 15th of June 2017.

\URL{https://www.tensorflow.org/}

\subsection{Keras}
A deep learning library which builds on top of Tensorflow.
Visited 4th of June 2017.

\URL{https://keras.io}

\pagebreak
\section{Appendix C: Figure sources}
\subsection{Figure \ref{fig:lstm}}
Source: Wikipedia.org

\URL{https://en.wikipedia.org/wiki/File:Peephole\_Long\_Short-Term\_Memory.svg}.

\end{document}
